The repository compares the two image-text matching models Negative Aware Attention Framework (NAAF) and Contrastive Language-Image Pretraining (CLIP) model on the subset of Flickr30K dataset. Given the image, NAAF and CLIP retrieve the best suitable text from the available. This work is based on the following two papers https://github.com/CrossmodalGroup/NAAF and https://github.com/openai/CLIP. Further, it implements the Stacked Cross Attention Network (SCAN) on the subset of Flickr30K data, which is implemented based on the work proposed by https://github.com/kuanghuei/SCAN. As part of image-text matching, I provided a report on Faster R-CNN and YOLO for extracting the objects from an image.
